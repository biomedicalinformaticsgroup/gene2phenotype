{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:51:23.725563Z",
     "start_time": "2021-11-04T15:51:23.510908Z"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez, Medline\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we can use the search terms provided to query pubmed using entrez esearch. \n",
    "# This will provide us with a list of pmids to retrieve in medline format\n",
    "# this function takes the search terms in the query string and returns a dictionary \n",
    "# the dict keys are: the date of serach, the query string used, the total count returned and the list of pmids \n",
    "def search_terms_to_pmid_list(query_string, email, api_key):\n",
    "    # set the entrez variables which are set up in advance for each group/project\n",
    "    Entrez.email = email\n",
    "    Entrez.tool = \"genepopi_search_developer\"\n",
    "    Entrez.api_key = api_key    \n",
    "    # get todays date \n",
    "    date = datetime.datetime.today()\n",
    "    date = f'{date.year}_{date.month}_{date.day}_{date.hour}_{date.minute}'\n",
    "    # send the query string to entrez esearch with the retmax at 90,000\n",
    "    # note if the number of returned records is > retmax we will need to iterate over the search results using a moving start and end point.\n",
    "    search_results = Entrez.read(Entrez.esearch(db=\"pubmed\", term = query_string, retmax=90000))\n",
    "    # We can then save the total count returned and pmid list to new variables\n",
    "    t_count = int(search_results['Count'])\n",
    "    pmids = list(search_results['IdList'])\n",
    "    \n",
    "    # construct the output dict\n",
    "    results_d = {'date':date, 'search_term':query_string, 'total_count':t_count, 'pmids':pmids}\n",
    "    \n",
    "    # check if we need to batch the pmid list due to limit on retreival set to 90000\n",
    "    if t_count > 90000:\n",
    "        print(\"Your search query has returned more than 90,000 results\\nWe will need to batch the pmid retrieval\")\n",
    "        # here's the batching process\n",
    "        start = len(pmids)\n",
    "        # set a while loop based on the return valeu\n",
    "        while start != t_count:\n",
    "            start+=1\n",
    "            # we now perform the same step as before and retrieve the pmids in bathces\n",
    "            search_results = Entrez.read(Entrez.esearch(db=\"pubmed\", term = query_string, retmax=90000, retstart = start))\n",
    "            # We can then save the total count returned and pmid list to new variables\n",
    "            pmids = list(search_results['IdList'])\n",
    "        \n",
    "            # update the output dict to add on the new pmids\n",
    "            previous = results_d['pmids']\n",
    "            previous.extend(pmids)\n",
    "                        \n",
    "            results_d.update({'pmids':previous})\n",
    "            # increment the start point for the batch retrieval\n",
    "            start += len(pmids)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # save the output dictionary for our records of what terms used and number of records returned for a given date.\n",
    "    pickle.dump(results_d, open(f'./{date}.p', 'wb'))\n",
    "\n",
    "    return results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:51:25.476309Z",
     "start_time": "2021-11-04T15:51:25.474250Z"
    }
   },
   "outputs": [],
   "source": [
    "# set your email\n",
    "email = 'michael.yates@ed.ac.uk'\n",
    "# add your NCBI api_key - **** register your email and generate an api key here https://www.ncbi.nlm.nih.gov/account/settings/\n",
    "api_key = '00ec1cf5351024793932907824e8b1ea1408'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T15:51:27.344482Z",
     "start_time": "2021-11-04T15:51:27.252569Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the ddg2p genes list\n",
    "ddg2p_df = pd.read_csv('DDG2P_6_7_2021.csv')\n",
    "# save the gene symbol to a list \n",
    "genes = list(set(ddg2p_df['gene symbol']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gene symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can store the output from the search as a dictionrary then dataframe\n",
    "gene_d = {}\n",
    "# loop through each gene symbol and search pubmed\n",
    "for gene in genes:\n",
    "    results_d = search_terms_to_pmid_list(gene, email, api_key)\n",
    "    # append the results to our holding dictionary\n",
    "    gene_d.update({gene:{'count':results_d['total_count'], 'pmids':results_d['pmids']}})\n",
    "    \n",
    "    # now lets make each gene's pmids as a text string to file\n",
    "    pmids_str = (',').join(results_d['pmids'])\n",
    "    with open(f'{gene}_pmids.txt', 'w+') as file:\n",
    "        file.write(pmids_str)\n",
    "        \n",
    "# now build the df to look at the counts and pmids quickly\n",
    "gene_df = pd.DataFrame.from_dict(gene_d, orient = 'index')\n",
    "# save the gene df to the working dir\n",
    "pickle.dump(gene_df, open('gene_df.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary step\n",
    "print(f'there are {gene_df[\"count\"].sum()} total pmids')\n",
    "print(f'the median pmids per gene is {gene_df[\"count\"].median()}')\n",
    "print(f'there are {len(gene_df[gene_df[\"count\"]==0])} genes with 0 pmid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gene symbol[ti] - search but only looking in the title field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ammend the genes lsit to add the title suffix\n",
    "genes_ti = [f'{i}[TI]' for i in genes]\n",
    "\n",
    "# we can store the output from the search as a dictionrary then dataframe\n",
    "gene_ti_d = {}\n",
    "for gene in genes_ti:\n",
    "    results_d = search_terms_to_pmid_list(gene, email, api_key)\n",
    "    gene_ti_d.update({gene:{'count':results_d['total_count'], 'pmids':results_d['pmids']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build the df to look at the counts and pmids quickly\n",
    "gene_ti_df = pd.DataFrame.from_dict(gene_ti_d, orient = 'index')\n",
    "pickle.dump(gene_ti_df, open('../cadmus/ddg2p_diagnosed/gene_ti_df.p', 'wb'))\n",
    "gene_ti_df.sort_values(by='count',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the cadmus pmids and save as a string.\n",
    "all_pmids = []\n",
    "for pmids in gene_ti_df['pmids']:\n",
    "    all_pmids.extend(pmids)\n",
    "print(f'there are {len(all_pmids)} pmids found from our {len(gene_ti_df)} gene search')\n",
    "\n",
    "cadmus_pmids = list(set(all_pmids))\n",
    "print(f'There are {len(cadmus_pmids)} unique pmids to get metadata for')\n",
    "\n",
    "with open('../cadmus/ddg2p_diagnosed/cadmus_ti_pmids.txt', 'w+') as file:\n",
    "    file.write(','.join(cadmus_pmids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gene symbol [tiab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can ammend the search string to increase the specificity, looking in certain fields. \n",
    "# for this example we'll be looking in the title and abstract fileds ([TIAB])\n",
    "genes_tiab = [f'{i}[TIAB]' for i in genes]  \n",
    "\n",
    "# we can store the output from the search as a dictionrary then dataframe\n",
    "gene_tiab_d = {}\n",
    "for gene in genes_tiab:\n",
    "    results_d = search_terms_to_pmid_list(gene, email, api_key)\n",
    "    gene_tiab_d.update({gene:{'count':results_d['total_count'], 'pmids':results_d['pmids']}})\n",
    "    \n",
    "    # now lets make each gene's pmids as a text string to file\n",
    "    pmids_str = (',').join(results_d['pmids'])\n",
    "    with open(f'{gene}_tiab_pmids.txt', 'w+') as file:\n",
    "        file.write(pmids_str)\n",
    "        \n",
    "# now build the df to look at the counts and pmids quickly\n",
    "gene_tiab_df = pd.DataFrame.from_dict(gene_tiab_d, orient = 'index')\n",
    "pickle.dump(gene_tiab_df, open('gene_tiab_df.p', 'wb'))\n",
    "gene_tiab_df.sort_values(by='count',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique pmids for a cadmus search and save as a string text file.\n",
    "all_pmids = []\n",
    "for pmids in gene_tiab_df['pmids']:\n",
    "    all_pmids.extend(pmids)\n",
    "print(f'there are {len(all_pmids)} pmids found from our {len(gene_tiab_df)} gene search')\n",
    "# exclude duplicates but using set function\n",
    "cadmus_pmids = list(set(all_pmids))\n",
    "print(f'There are {len(cadmus_pmids)} unique pmids to get metadata for')\n",
    "# write top file\n",
    "with open('cadmus_tiab_pmids.txt', 'w+') as file:\n",
    "    file.write(','.join(cadmus_pmids))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ebcaf47573c63dc6b29179b08c22882e80c076082f9c55472dd61f32ef8cb2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:gopher]",
   "language": "python",
   "name": "conda-env-gopher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T10:30:42.746220Z",
     "start_time": "2021-11-23T10:30:42.742446Z"
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez, Medline\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T10:39:46.203230Z",
     "start_time": "2021-11-23T10:39:46.195642Z"
    }
   },
   "outputs": [],
   "source": [
    "def pmids_to_medline_file(file_name, pmid_list, email, api_key): \n",
    "    \n",
    "    # this function uses a list of pmids to return medline files from NCBI using epost and efetch\n",
    "    # the output is a file to parse using medline parser\n",
    "    \n",
    "    # The PMID list may be provided directly or it may have be retrieved by using a pubmed search. \n",
    "    # if the pubmed search was used then the date_name can be parsed from the results_d in the previous function\n",
    "    # other wise, the PMID list has be provided from another source and we can need to make a new date_name\n",
    "    \n",
    "    # set the entrez variable set up for the group/project\n",
    "    Entrez.email = email\n",
    "    Entrez.tool = \"genepopi_search_developer\"\n",
    "    Entrez.api_key = api_key    \n",
    "\n",
    "\n",
    "    # post a joined list of the new pmids to the NCBI history server and save the search results.\n",
    "    # NCBI's history server allows you to post once and then iteratively retrieve records without reposting\n",
    "    # it also works better than sending a long URL (full of the pmids) which risks breaking\n",
    "\n",
    "    # to do this we need to record the webenv and query_key to use in out e-fetch request\n",
    "    \n",
    "    # remove all duplicate pmids\n",
    "    new_pmids = list(set(pmid_list))\n",
    "    search_results = Entrez.read(Entrez.epost(db=\"pubmed\", id=\",\".join(new_pmids)))\n",
    "\n",
    "    web_env = search_results['WebEnv']\n",
    "    query_key = search_results['QueryKey']\n",
    "\n",
    "    # if the total count is greated than the max retieval then we will need to retrieve in batches.\n",
    "    t_count = len(new_pmids)\n",
    "    # 500 is the max batch size\n",
    "    batch_size = 500\n",
    "\n",
    "    # set the file name to store the medline records, i am using the date searched but you can change the name variable above to whatever you like\n",
    "    out_handle = open(f\"medline_files/{file_name}.txt\", \"a+\")\n",
    "\n",
    "    # now lets use an efetch loop to retrieve medline records from our pmid list \n",
    "    # the start will be set by jumping from 0 to the final counts, in increments of the batch size\n",
    "    for start in range(0,t_count,batch_size):\n",
    "        # set the end number of retieval to be the smallest out of the total or start plus batch number of \n",
    "        end = min(t_count, start+batch_size)\n",
    "        # give some feedback on the process\n",
    "        print(f\"Going to download record {start+1} to {end} out of {t_count} for search: {file_name}\")\n",
    "        # occasional server errors should be expected, this try:except block will allow 3 attempts to download each batch\n",
    "        attempt = 0\n",
    "        while attempt <= 3:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                # send a request to efetch pubmed db in the medline format, setting the start and end record, according to the pmid post on the history server \n",
    "                fetch_handle = Entrez.efetch(db=\"pubmed\",rettype=\"medline\",\n",
    "                                             retmode=\"text\",retstart=start,\n",
    "                                             retmax=batch_size,\n",
    "                                             webenv=web_env,\n",
    "                                             query_key=query_key)\n",
    "                attempt = 4\n",
    "            # the except block will occur when there has been an error\n",
    "            except:\n",
    "                pass\n",
    "        # the data read in from the respons is then written to the outhandle until the loop is complete and the handle is then closed.\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        out_handle.write(data)\n",
    "    out_handle.close()\n",
    "    print(f'Job Complete, output file path = medline_files/{file_name}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T10:40:59.473431Z",
     "start_time": "2021-11-23T10:39:46.981923Z"
    }
   },
   "outputs": [],
   "source": [
    "# set your email\n",
    "email = 'email'\n",
    "# add your NCBI api_key - **** register your email and generate an api key here https://www.ncbi.nlm.nih.gov/account/settings/\n",
    "api_key = 'api_key'\n",
    "\n",
    "# set the name of the output text file\n",
    "file_name = 'medline'\n",
    "\n",
    "\n",
    "# get the pmid list to search\n",
    "pmid_list = list(set('previously_defined_list_of_pmids'))\n",
    "print(f'The input file has {len(pmid_list)} pmids to get the metadata for')\n",
    "\n",
    "pmids_to_medline_file(file_name, pmid_list, email, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T10:40:59.476884Z",
     "start_time": "2021-11-23T10:40:59.474934Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we can read in the medline file to create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T11:27:48.169852Z",
     "start_time": "2021-11-23T11:27:48.159706Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_medline_doi(record):\n",
    "    # the record we refer to is a bio.medline.record, effectively a python dictionary\n",
    "    # getting a doi is valuable and we need to try a bit harder to find one\n",
    "    doi = None\n",
    "    # look and see if the LID field is present in the record                        \n",
    "    LID = record.get('LID')\n",
    "    if LID:\n",
    "        # iterate through the LID looking for 'doi'\n",
    "        for val in LID:\n",
    "            if 'doi' in val:\n",
    "                # split the string to get rid of ' [doi]' and keep the plain doi string\n",
    "                doi = val.split()[0]\n",
    "    # if that didn't work then look in the AID Field and do the same\n",
    "    if doi == None:\n",
    "        AID = record.get(\"AID\")\n",
    "        if AID:\n",
    "            for val in AID:\n",
    "                if 'doi' in val:\n",
    "                    doi = val.split()[0]\n",
    "    # sometimes the doi is only present in the SO section and needs to be parsed out of the citation\n",
    "    if doi == None:\n",
    "        SO = record.get('SO')\n",
    "        if SO:\n",
    "            if 'doi' in SO:\n",
    "                # now we need to remove all the surround text and full stop from the end   \n",
    "                # use regular expression to locate the \"doi: xkkxjwkdjdfksfd\" section and cut it out as a string\n",
    "                doi = re.findall(r'doi: \\S+', SO)\n",
    "                # remove 'doi' from the string\n",
    "                doi = doi[0].replace('doi: ', '')\n",
    "                # check to see if there is a full stop at the end of the string (doi's dont end in full stops)\n",
    "                if doi[-1] == '.':\n",
    "                    doi = doi[:-1] \n",
    "    # now save whatever we got back, otherwise save the default None            \n",
    "    return doi\n",
    "\n",
    "def creation_retrieved_df(medline_file_name):\n",
    "    # as input we provide the name of the medline file for parsing out all the individual records\n",
    "    # now lets run the parser for all the new records \n",
    "    # as each record is parsed, we give it a unique index (hexidecimal string)\n",
    "    # each medline record is then written to file(named the same as the unique index)so that we can find the metadata easily if we want to parse out other fields\n",
    "    # the medline records are then added to a dictionary which will become the basic retrieved_df.\n",
    "    \n",
    "    # set the input file\n",
    "    in_file = medline_file_name\n",
    "    \n",
    "    # our main output will be a dictionary ready to be converted into a retrieved df\n",
    "    parse_d = {}\n",
    "\n",
    "    #  read in the text file using Medline Parser from biopython\n",
    "    with open(in_file, 'r') as handle:\n",
    "        # biopython provides a medline parser so that each record is imported as a dictionary to extract from\n",
    "        records = Medline.parse(handle)\n",
    "\n",
    "        # loop through each record creating a set of the most important variables\n",
    "        for record in records:\n",
    "            # create a hexidecimal unique id for the record\n",
    "            index = str(uuid.uuid4().hex)\n",
    "\n",
    "\n",
    "            # we use the get() function for a dictionary to search each field.\n",
    "            # if the field is populated add the value, else, add 'None'\n",
    "            pmid = record.get('PMID')\n",
    "            pmcid = record.get('PMC')\n",
    "            title = record.get('TI')\n",
    "            abstract = record.get('AB')\n",
    "            authors = record.get('AU')\n",
    "            journal_title = record.get('JT')\n",
    "            pub_type = record.get('PT')\n",
    "            issn = record.get('IS')\n",
    "            gene = record.get('GS')\n",
    "            mesh = record.get('MH')\n",
    "            comment_on = record.get('CON')\n",
    "            erratum_for = record.get('EFR')\n",
    "            correct_repub = record.get('CRF')\n",
    "\n",
    "            # use our function above to get a datetime obj for the pdat provided (or None)                               \n",
    "            dt_pdat = record.get('DP')\n",
    "            \n",
    "            # now get the doi using the function above          \n",
    "            doi = get_medline_doi(record)\n",
    "\n",
    "            # now we save all the variables to the parse dictionary\n",
    "            parse_d.update({index:{'pmid': pmid,\n",
    "                                'pmcid':pmcid,\n",
    "                                'title': title,\n",
    "                                'abstract': abstract,\n",
    "                                'authors':authors,\n",
    "                                'journal':journal_title,\n",
    "                                'pub_type':pub_type,\n",
    "                                'pub_date':dt_pdat,\n",
    "                                'doi': doi,\n",
    "                                'issn':issn,\n",
    "                                   'gene_meta':gene,\n",
    "                                   'mesh':mesh,\n",
    "                                   'comment_on':comment_on,\n",
    "                                   'erratum_for':erratum_for,\n",
    "                                   'correct_repub':correct_repub}})\n",
    "\n",
    "    pm_df = pd.DataFrame.from_dict(parse_d, orient= 'index')\n",
    "\n",
    "    print('Process Complete')\n",
    "    return pm_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T11:27:52.995767Z",
     "start_time": "2021-11-23T11:27:52.035975Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets build our metadata dataframe\n",
    "meta_df = creation_retrieved_df('medline_files/medline.txt')\n",
    "# now write the dataframe to file\n",
    "meta_df.to_csv('medline_files/meta_df.tsv', sep = '\\t', header = True, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T11:27:54.195326Z",
     "start_time": "2021-11-23T11:27:54.124392Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(meta_df,open('metadata_df.p','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
